#!/usr/bin/env python3
"""
Probability Tree Implementation
==============================

Optimized probability tree for semantic distance scoring using distilGPT-2.
Prioritizes memory efficiency and fast lookup while maintaining mathematical accuracy.
"""

import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class ProbabilityMetadata:
    """Optimized metadata for probability recovery and creativity scoring."""
    org_max: float          # Original max probability in full vocab
    val_prb_sum: float      # Sum of probabilities for valid tokens
    max_dep: int            # Maximum depth of token sequences
    
    def __post_init__(self):
        # Validate mathematical consistency
        # After normalization, val_prb_sum can be 1.0, which is greater than org_max
        # This is mathematically correct - we only check that individual probs don't exceed org_max
        assert self.max_dep >= 0, f"Max depth must be non-negative: {self.max_dep}"

@dataclass
class ChildNode:
    """Optimized child node structure for recursive probability trees."""
    probability: float       # Probability for this token
    remaining_sequences: List[List[int]]  # Remaining token sequences
    child_prb: 'ProbabilityNode'  # Child probability structure
    
    def __post_init__(self):
        assert 0.0 <= self.probability <= 1.0, f"Probability must be in [0,1]: {self.probability}"

@dataclass
class ProbabilityNode:
    """Optimized probability node with sparse array and metadata."""
    val: List[List[int]]    # Valid token sequences
    prb: Dict[int, Union[float, ChildNode]]  # Sparse array: token_idx -> probability or child
    dat: ProbabilityMetadata # Recovery metadata
    
    def __post_init__(self):
        # Validate sparse array consistency
        for token_idx, value in self.prb.items():
            if isinstance(value, ChildNode):
                assert 0.0 <= value.probability <= 1.0
            else:
                assert 0.0 <= value <= 1.0

@dataclass
class WordProbabilityTree:
    """Complete probability tree for a start word."""
    frq: int                # Word frequency
    ana: ProbabilityNode    # Anagram transformations
    olo: Dict[str, ProbabilityNode]  # One-letter-off transformations
    rhy: Dict[str, ProbabilityNode]  # Rhyme transformations
    
    def __post_init__(self):
        # Validate transformation categories
        expected_olo = {'ola', 'olr', 'olx'}
        expected_rhy = {'prf', 'rch', 'sln'}
        assert set(self.olo.keys()) == expected_olo, f"Missing OLO categories: {set(self.olo.keys())}"
        assert set(self.rhy.keys()) == expected_rhy, f"Missing rhyme categories: {set(self.rhy.keys())}"

class ProbabilityTreeBuilder:
    """Optimized builder for probability trees with lazy caching."""
    
    def __init__(self, model, tokenizer, vocab_size: int):
        self.model = model
        self.tokenizer = tokenizer
        self.vocab_size = vocab_size
        self._cache = {}  # Memory-efficient cache: start_word -> WordProbabilityTree
        
    def get_or_build_tree(self, start_word: str, valid_words: Dict[str, List[List[int]]]) -> WordProbabilityTree:
        """
        Lazy caching: build tree only when needed.
        
        Args:
            start_word: The word to build tree for
            valid_words: Dict mapping category -> token sequences
            
        Returns:
            Cached or newly built WordProbabilityTree
        """
        if start_word in self._cache:
            logger.debug(f"üì¶ Using cached tree for '{start_word}'")
            return self._cache[start_word]
        
        logger.info(f"üîÑ Building probability tree for '{start_word}'")
        tree = self._build_complete_tree(start_word, valid_words)
        
        # Validate tree before caching
        if tree is None:
            logger.error(f"‚ùå Tree building failed for '{start_word}'")
            return None
            
        if validate_probability_tree(tree):
            self._cache[start_word] = tree
            logger.info(f"‚úÖ Successfully built and validated tree for '{start_word}'")
            return tree
        else:
            logger.error(f"‚ùå Tree validation failed for '{start_word}'")
            return None
    
    def _build_complete_tree(self, start_word: str, valid_words: Dict[str, List[List[int]]]) -> WordProbabilityTree:
        """Build complete probability tree with all transformation categories."""
        
        # Initialize probability vector cache to avoid redundant model calls
        cached_prob_vectors = {}
        
        # Get word frequency (placeholder - would use wordfreq library)
        frq = self._get_word_frequency(start_word)
        
        # Build each transformation category with cache
        ana_tree = self._build_probability_node(start_word, valid_words.get('ana', []), 'anagram', cached_prob_vectors)
        olo_trees = {
            'ola': self._build_probability_node(start_word, valid_words.get('ola', []), 'one-letter-added', cached_prob_vectors),
            'olr': self._build_probability_node(start_word, valid_words.get('olr', []), 'one-letter-removed', cached_prob_vectors),
            'olx': self._build_probability_node(start_word, valid_words.get('olx', []), 'one-letter-changed', cached_prob_vectors)
        }
        rhy_trees = {
            'prf': self._build_probability_node(start_word, valid_words.get('prf', []), 'perfect-rhyme', cached_prob_vectors),
            'rch': self._build_probability_node(start_word, valid_words.get('rch', []), 'rich-rhyme', cached_prob_vectors),
            'sln': self._build_probability_node(start_word, valid_words.get('sln', []), 'slant-rhyme', cached_prob_vectors)
        }
        
        # Log category statistics
        logger.info(f"üìä Tree categories for '{start_word}':")
        logger.info(f"  - Anagrams: {len(valid_words.get('ana', []))} sequences")
        logger.info(f"  - OLO: {len(valid_words.get('ola', []))} + {len(valid_words.get('olr', []))} + {len(valid_words.get('olx', []))} sequences")
        logger.info(f"  - Rhymes: {len(valid_words.get('prf', []))} + {len(valid_words.get('rch', []))} + {len(valid_words.get('sln', []))} sequences")
        
        return WordProbabilityTree(
            frq=frq,
            ana=ana_tree,
            olo=olo_trees,
            rhy=rhy_trees
        )
    

    
    def _build_probability_node(self, start_word: str, token_sequences: List[List[int]], category: str, 
                               cached_prob_vectors: Dict[str, List[float]] = None) -> ProbabilityNode:
        """
        Build optimized probability node with sparse array and child nodes.
        
        Args:
            start_word: The original word
            token_sequences: List of valid token sequences for this category
            category: Transformation category for context
            cached_prob_vectors: Pre-cached probability vectors to avoid redundant model calls
            
        Returns:
            ProbabilityNode with sparse array and metadata
        """
        import time
        start_time = time.time()
        
        if not token_sequences:
            # Empty category - return minimal node with null sentinel (NO MODEL CALL)
            logger.debug(f"‚è≠Ô∏è  Skipping model call for empty category: {category}")
            return ProbabilityNode(
                val=[None],  # Null sentinel like null byte in C
                prb={},
                dat=ProbabilityMetadata(org_max=0.0, val_prb_sum=0.0, max_dep=0)
            )
        
        # Group sequences by first token for efficiency
        token_groups = self._group_sequences_by_first_token(token_sequences)
        grouping_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è  Category '{category}': Grouping took {grouping_time:.3f}s")
        
        # Get full probability array ONCE for this context (avoid repeated model calls)
        model_start = time.time()
        full_prompt = f"{start_word} is a word that {category} with"
        
        # Use cached probability vector if available
        if cached_prob_vectors and full_prompt in cached_prob_vectors:
            probabilities = cached_prob_vectors[full_prompt]
            logger.debug(f"üì¶ Using cached probability vector for '{full_prompt}'")
        else:
            prob_vector_data = self.model.get_probability_vector(full_prompt)
            probability_vector = prob_vector_data["probability_vector"]
            
            # Handle both numpy arrays and lists
            if hasattr(probability_vector, 'tolist'):
                probabilities = probability_vector.tolist()
            else:
                probabilities = list(probability_vector)
            
            # Cache this probability vector
            if cached_prob_vectors is not None:
                cached_prob_vectors[full_prompt] = probabilities
        
        model_time = time.time() - model_start
        logger.info(f"‚è±Ô∏è  Category '{category}': Model call took {model_time:.3f}s")
        
        # Build sparse array and child nodes - ONLY store valid tokens with non-zero probabilities
        sparse_start = time.time()
        sparse_array = {}
        max_depth = max(len(seq) for seq in token_sequences)
        
        # ONLY iterate through valid tokens for this context (not the entire vocabulary)
        for token_idx, child_sequences in token_groups.items():
            # Get probability for this specific valid token from the full array
            if token_idx < len(probabilities):
                probability = probabilities[token_idx]
                
                # ONLY store if both: 1) token is valid for context AND 2) probability > 0
                if probability > 0:
                    if child_sequences and any(child_sequences):  # Has non-empty remainders
                        # Recursively build child node with sliced sequences as val
                        # Pass the cached probability vectors to avoid redundant model calls
                        child_node = self._build_probability_node(
                            start_word,
                            child_sequences,  # These are already sliced [1:] sequences
                            category,
                            cached_prob_vectors  # Pass cache to child
                        )
                        sparse_array[token_idx] = ChildNode(
                            probability=probability,
                            remaining_sequences=child_sequences,
                            child_prb=child_node
                        )
                    else:
                        # Terminal node - direct probability
                        sparse_array[token_idx] = probability
        
        sparse_time = time.time() - sparse_start
        logger.info(f"‚è±Ô∏è  Category '{category}': Sparse array building took {sparse_time:.3f}s")
        
        # Normalize probabilities to sum to 1.0
        norm_start = time.time()
        total_prob = sum(
            prob if isinstance(prob, float) else prob.probability
            for prob in sparse_array.values()
        )
        
        if total_prob > 0:
            # Renormalize all probabilities
            for token_idx in sparse_array:
                if isinstance(sparse_array[token_idx], float):
                    sparse_array[token_idx] /= total_prob
                else:
                    sparse_array[token_idx].probability /= total_prob
        
        norm_time = time.time() - norm_start
        logger.info(f"‚è±Ô∏è  Category '{category}': Normalization took {norm_time:.3f}s")
        
        # Calculate metadata for recovery (use original model max, not stored max)
        meta_start = time.time()
        valid_probs = [prob if isinstance(prob, float) else prob.probability 
                      for prob in sparse_array.values()]
        org_max = max(probabilities) if probabilities else 0.0  # Use original model max
        val_prb_sum = sum(valid_probs) if valid_probs else 0.0
        
        total_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è  Category '{category}': Total build time {total_time:.3f}s")
        
        return ProbabilityNode(
            val=token_sequences,  # Keep original sequences for this context level
            prb=sparse_array,
            dat=ProbabilityMetadata(
                org_max=org_max,
                val_prb_sum=val_prb_sum,
                max_dep=max_depth
            )
        )
    
    def _build_probability_node_with_cache(self, start_word: str, token_sequences: List[List[int]], 
                                         category: str, cached_prob_vector: dict) -> ProbabilityNode:
        """
        Build probability node using cached probability vector (no model calls).
        
        Args:
            start_word: The original word
            token_sequences: List of valid token sequences for this category
            category: Transformation category for context
            cached_prob_vector: Pre-calculated probability vector
            
        Returns:
            ProbabilityNode with sparse array and metadata
        """
        if not token_sequences:
            return ProbabilityNode(
                val=[None],
                prb={},
                dat=ProbabilityMetadata(org_max=0.0, val_prb_sum=0.0, max_dep=0)
            )
        
        # Group sequences by first token
        token_groups = self._group_sequences_by_first_token(token_sequences)
        
        # Use cached probability vector
        probability_vector = cached_prob_vector["probability_vector"]
        if hasattr(probability_vector, 'tolist'):
            probabilities = probability_vector.tolist()
        else:
            probabilities = list(probability_vector)
        
        # Build sparse array using cached probabilities
        sparse_array = {}
        max_depth = max(len(seq) for seq in token_sequences)
        
        for token_idx, child_sequences in token_groups.items():
            if token_idx < len(probabilities):
                probability = probabilities[token_idx]
                
                if probability > 0:
                    if child_sequences and any(child_sequences):
                        # For deeper levels, we could implement more caching, but for now
                        # we'll use the simple approach since most sequences are short
                        child_node = self._build_probability_node_with_cache(
                            start_word,
                            child_sequences,
                            category,
                            {"probability_vector": []}  # Empty for terminal nodes
                        )
                        sparse_array[token_idx] = ChildNode(
                            probability=probability,
                            remaining_sequences=child_sequences,
                            child_prb=child_node
                        )
                    else:
                        sparse_array[token_idx] = probability
        
        # Normalize probabilities
        total_prob = sum(
            prob if isinstance(prob, float) else prob.probability
            for prob in sparse_array.values()
        )
        
        if total_prob > 0:
            for token_idx in sparse_array:
                if isinstance(sparse_array[token_idx], float):
                    sparse_array[token_idx] /= total_prob
                else:
                    sparse_array[token_idx].probability /= total_prob
        
        # Calculate metadata
        valid_probs = [prob if isinstance(prob, float) else prob.probability 
                      for prob in sparse_array.values()]
        org_max = max(probabilities) if probabilities else 0.0
        val_prb_sum = sum(valid_probs) if valid_probs else 0.0
        
        return ProbabilityNode(
            val=token_sequences,
            prb=sparse_array,
            dat=ProbabilityMetadata(
                org_max=org_max,
                val_prb_sum=val_prb_sum,
                max_dep=max_depth
            )
        )
    
    def _group_sequences_by_first_token(self, sequences: List[List[int]]) -> Dict[int, List[List[int]]]:
        """Optimized grouping: sequences sharing same first token."""
        groups = {}
        for sequence in sequences:
            if sequence:  # Non-empty sequence
                first_token = sequence[0]
                if first_token not in groups:
                    groups[first_token] = []
                groups[first_token].append(sequence[1:])  # Store remainder
        return groups
    
    def _get_word_frequency(self, word: str) -> int:
        """Get word frequency (placeholder - would use wordfreq library)."""
        # Placeholder: return random frequency
        # In real implementation, would use wordfreq.word_frequency(word, 'en')
        return 50  # Placeholder frequency

class ProbabilityTreeLookup:
    """Optimized lookup engine for probability trees."""
    
    @staticmethod
    def get_sequence_probability(tree: WordProbabilityTree, category: str, subcategory: str, 
                                token_sequence: List[int]) -> float:
        """
        Get probability for complete token sequence.
        
        Args:
            tree: WordProbabilityTree
            category: 'ana', 'olo', or 'rhy'
            subcategory: Specific transformation type
            token_sequence: Complete token sequence to score
            
        Returns:
            Probability for the sequence
        """
        # Get the appropriate probability node
        if category == 'ana':
            node = tree.ana
        elif category == 'olo':
            node = tree.olo[subcategory]
        elif category == 'rhy':
            node = tree.rhy[subcategory]
        else:
            raise ValueError(f"Invalid category: {category}")
        
        return ProbabilityTreeLookup._traverse_probability_node(node, token_sequence)
    
    @staticmethod
    def _traverse_probability_node(node: ProbabilityNode, token_sequence: List[int]) -> float:
        """Recursively traverse probability node to get sequence probability."""
        if not token_sequence:
            return 1.0  # Empty sequence has probability 1.0
        
        current_node = node
        total_prob = 1.0
        
        for token_id in token_sequence:
            if token_id not in current_node.prb:
                return 0.0  # Invalid sequence
            
            # Extract probability and move to child
            prob_value = current_node.prb[token_id]
            
            if isinstance(prob_value, float):
                # Terminal node
                total_prob *= prob_value
                break
            else:
                # Child node - continue traversal
                total_prob *= prob_value.probability
                current_node = prob_value.child_prb
        
        return total_prob
    
    @staticmethod
    def get_creativity_score(tree: WordProbabilityTree, category: str, subcategory: str,
                            token_sequence: List[int]) -> float:
        """
        Get creativity score relative to model's original preferences.
        
        Args:
            tree: WordProbabilityTree
            category: 'ana', 'olo', or 'rhy'
            subcategory: Specific transformation type
            token_sequence: Complete token sequence to score
            
        Returns:
            Creativity score (0.0 = predictable, 1.0 = creative)
        """
        sequence_prob = ProbabilityTreeLookup.get_sequence_probability(
            tree, category, subcategory, token_sequence
        )
        
        if sequence_prob == 0.0:
            return 0.0
        
        # Calculate total renormalization factor by traversing path
        if category == 'ana':
            node = tree.ana
        elif category == 'olo':
            node = tree.olo[subcategory]
        elif category == 'rhy':
            node = tree.rhy[subcategory]
        else:
            raise ValueError(f"Invalid category: {category}")
        
        total_renorm_factor = 1.0
        current_node = node
        
        for token_id in token_sequence:
            if token_id not in current_node.prb:
                return 0.0
            
            current_dat = current_node.dat
            total_renorm_factor *= current_dat.val_prb_sum
            
            prob_value = current_node.prb[token_id]
            if isinstance(prob_value, ChildNode):
                current_node = prob_value.child_prb
            else:
                break  # Terminal node
        
        # Convert back to original probability space
        original_prob = sequence_prob * total_renorm_factor
        original_max = node.dat.org_max
        
        return original_prob / original_max if original_max > 0 else 0.0

def validate_probability_tree(tree: WordProbabilityTree) -> bool:
    """Validate mathematical consistency of probability tree."""
    try:
        # Validate each transformation category
        _validate_node(tree.ana, "ana")
        
        for subcategory, node in tree.olo.items():
            _validate_node(node, f"olo.{subcategory}")
        
        for subcategory, node in tree.rhy.items():
            _validate_node(node, f"rhy.{subcategory}")
        
        logger.info("‚úÖ Probability tree validation passed")
        return True
        
    except AssertionError as e:
        logger.error(f"‚ùå Probability tree validation failed: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Probability tree validation error: {e}")
        return False

def _validate_node(node: ProbabilityNode, path: str):
    """Recursively validate probability node."""
    prb_data = node.prb
    
    # Handle empty categories with null sentinel
    if node.val == [None]:
        logger.debug(f"Empty category {path} with null sentinel - skipping validation")
        return
    
    # Handle empty categories (no valid words)
    if not prb_data:
        logger.debug(f"Empty category {path} - skipping validation")
        return
    
    # Check renormalization
    total_prob = sum(
        prob if isinstance(prob, float) else prob.probability
        for prob in prb_data.values()
    )
    
    # Allow empty categories (total_prob = 0) or properly normalized categories (total_prob ‚âà 1)
    if total_prob > 0:
        # Be more tolerant of small numerical errors
        if abs(total_prob - 1.0) >= 1e-3:
            logger.warning(f"‚ö†Ô∏è  Probabilities at {path} don't sum to 1: {total_prob}")
            # Try to fix by renormalizing
            for token_idx in prb_data:
                if isinstance(prb_data[token_idx], float):
                    prb_data[token_idx] /= total_prob
                else:
                    prb_data[token_idx].probability /= total_prob
            logger.info(f"‚úÖ Renormalized probabilities at {path}")
        else:
            logger.debug(f"‚úÖ Probabilities at {path} sum to {total_prob:.6f}")
    
    # Check metadata consistency
    original_max = node.dat.org_max
    valid_sum = node.dat.val_prb_sum
    if valid_sum > 0:
        # After normalization, valid_sum can be 1.0, which is greater than original_max
        # This is mathematically correct - we only check that individual probs don't exceed original_max
        # assert valid_sum <= original_max, f"Valid sum {valid_sum} exceeds original max {original_max} at {path}"
        pass  # Skip this validation as it's not mathematically sound after normalization
    
    # Recursively validate children
    for prob_value in prb_data.values():
        if isinstance(prob_value, ChildNode):
            _validate_node(prob_value.child_prb, f"{path}.child") 